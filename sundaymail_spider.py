# -*- coding: utf-8 -*-
"""sundaymail_spider.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUiN6DwWjqwm-DRrq3YL7ynUq04WB2DT
"""

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd

class SundayMailSpider:
    def __init__(self):
        """Initializes the spider with a session."""
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})

    def scrape_sundaymail_articles(self, url, category):
        """Scrapes articles from the given URL under the specified category."""
        response = self.session.get(url)
        page = bs(response.content, 'html.parser', from_encoding='utf_8_sig')

        articles = []
        main_container = page.find('div', id='sirius-article-list')

        if main_container:
            article_cards = main_container.find_all('div', class_='hentry sirius-card')
            for card in article_cards:
                title_link = card.find('a')
                if title_link:
                    title = title_link.get('title')
                    url = title_link.get('href')
                    summary = card.find('p', class_='entry-summary')
                    story_text = summary.get_text(strip=True) if summary else ''

                    articles.append({
                        'category': category,
                        'title': title,
                        'story': story_text,
                        'url': url
                    })

        return articles

    def run(self):
        """Executes the main scraping logic for all categories."""
        categories_urls = {
            'Business': ['https://www.sundaymail.co.zw/category/business', 'https://www.sundaymail.co.zw/category/business/page/2', 'https://www.sundaymail.co.zw/category/business/page/3', 'https://www.sundaymail.co.zw/category/business/page/4', 'https://www.sundaymail.co.zw/category/business/page/5'],
            'Politics': ['https://www.sundaymail.co.zw/?s=politics', 'https://www.sundaymail.co.zw/page/2?s=politics', 'https://www.sundaymail.co.zw/page/3?s=politics', 'https://www.sundaymail.co.zw/page/4?s=politics', 'https://www.sundaymail.co.zw/page/5?s=politics'],
            'Arts & Culture': ['https://www.sundaymail.co.zw/category/society/', 'https://www.sundaymail.co.zw/category/society/page/2', 'https://www.sundaymail.co.zw/category/society/page/3', 'https://www.sundaymail.co.zw/category/society/page/4', 'https://www.sundaymail.co.zw/category/society/page/5'],
            'Sports': ['https://www.sundaymail.co.zw/category/sports/', 'https://www.sundaymail.co.zw/category/sports/page/2', 'https://www.sundaymail.co.zw/category/sports/page/3', 'https://www.sundaymail.co.zw/category/sports/page/4', 'https://www.sundaymail.co.zw/category/sports/page/5'],
        }

        all_articles = []
        for category, urls in categories_urls.items():
            for url in urls:
                print(f"Scraping {category} articles from {url}...")
                all_articles += self.scrape_sundaymail_articles(url, category)

        # Convert the list of articles to a DataFrame and save it
        df_articles = pd.DataFrame(all_articles)
        print(df_articles)
        df_articles.to_csv('sundaymail_articles_combined.csv', index=False)


spider = SundayMailSpider()
spider.run()

