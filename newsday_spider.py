# -*- coding: utf-8 -*-
"""newsday_spider.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_ctBHyRE0Cne0eRK6N4x2kiaql7bw3KP
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

class NewsdaySpider:
    def __init__(self):
        """Initializes the spider with a requests session."""
        self.session = requests.Session()

    def scrape_newsday_articles(self, url, category):
        """Scrapes articles from the given URL under the specified category."""
        response = self.session.get(url)
        response.raise_for_status()  # Ensure the request was successful
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = []
        sections = soup.find_all('div', class_='col-12')
        for section in sections:
            link = section.find('a', class_='text-dark')
            if link and link.get('href'):
                title_url = link.get('href')
                title = link.get_text(strip=True)

                story_div = section.find('div', class_='mb-3', recursive=False)
                story = story_div.get_text(strip=True) if story_div else ''

                articles.append({
                    'category': category,
                    'title': title,
                    'story': story,
                    'url': title_url
                })

        return articles

    def run(self):
        """Main execution method to scrape all categories."""
        categories_urls = {
            'Business': ['https://www.newsday.co.zw/category/4/business', 'https://www.newsday.co.zw/category/38/world-business', 'https://www.newsday.co.zw/category/15978/finance'],
            'Politics': ['https://www.newsday.co.zw/category/69/politics'],
            'Arts & Culture': ['https://www.newsday.co.zw/category/8/life-amp-style'],
            'Sports': ['https://www.newsday.co.zw/category/5/sport', 'https://www.newsday.co.zw/category/35/other-sport', 'https://www.newsday.co.zw/category/28/soccer', 'https://www.newsday.co.zw/category/29/rugby']
        }

        all_articles = []
        for category, urls in categories_urls.items():
            for url in urls:
                print(f"Scraping {category} articles from {url}...")
                all_articles += self.scrape_newsday_articles(url, category)

        # Convert the list of articles to a DataFrame and save it
        df_articles = pd.DataFrame(all_articles)
        print(df_articles)
        df_articles.to_csv('newsday_articles_comb.csv', index=False)


spider = NewsdaySpider()
spider.run()

